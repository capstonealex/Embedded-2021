{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5c3c5aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2\n",
    "import argparse\n",
    "import os.path\n",
    "import io\n",
    "import os\n",
    "import open3d as o3d\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "from object_detection.utils import config_util\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46aa3426",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_MODEL_NAME = 'my_ssd_resnet50_test_1' \n",
    "PRETRAINED_MODEL_NAME = 'ssd_resnet50_v1_fpn_640x640_coco17_tpu-8'\n",
    "PRETRAINED_MODEL_URL = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz'\n",
    "TF_RECORD_SCRIPT_NAME = 'generate_tfrecord.py'\n",
    "LABEL_MAP_NAME = 'label_map.pbtxt'\n",
    "\n",
    "paths = {\n",
    "    'WORKSPACE_PATH': os.path.join('Tensorflow', 'workspace'),\n",
    "    'SCRIPTS_PATH': os.path.join('Tensorflow','scripts'),\n",
    "    'APIMODEL_PATH': os.path.join('Tensorflow','models'),\n",
    "    'ANNOTATION_PATH': os.path.join('Tensorflow', 'workspace','annotations'),\n",
    "    'IMAGE_PATH': os.path.join('Tensorflow', 'workspace','images'),\n",
    "    'MODEL_PATH': os.path.join('Tensorflow', 'workspace','models'),\n",
    "    'PRETRAINED_MODEL_PATH': os.path.join('Tensorflow', 'workspace','pre-trained-models'),\n",
    "    'CHECKPOINT_PATH': os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME), \n",
    "    'OUTPUT_PATH': os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'export'), \n",
    "    'TFJS_PATH':os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'tfjsexport'), \n",
    "    'TFLITE_PATH':os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'tfliteexport'), \n",
    "    'PROTOC_PATH':os.path.join('Tensorflow','protoc')\n",
    " }\n",
    "\n",
    "files = {\n",
    "    'PIPELINE_CONFIG':os.path.join('Tensorflow', 'workspace','models', CUSTOM_MODEL_NAME, 'pipeline.config'),\n",
    "    'TF_RECORD_SCRIPT': os.path.join(paths['SCRIPTS_PATH'], TF_RECORD_SCRIPT_NAME), \n",
    "    'LABELMAP': os.path.join(paths['ANNOTATION_PATH'], LABEL_MAP_NAME)\n",
    "}\n",
    "\n",
    "# Load pipeline config and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(files['PIPELINE_CONFIG'])\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(paths['CHECKPOINT_PATH'], 'ckpt-17')).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n",
    "\n",
    "category_index = label_map_util.create_category_index_from_labelmap(files['LABELMAP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2099037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NumpyToPCD(xyz):\n",
    "    \"\"\"Function to convert numpy array to open3d point cloud \n",
    "        Input: xyz - numpy array\n",
    "        Output: pcd - Point Cloud data\"\"\"\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(xyz)\n",
    "\n",
    "    return pcd\n",
    "\n",
    "\n",
    "def detect_planes(input_pcd, min_ratio):\n",
    "    \"\"\"Function to detect multiple planes using RANSAC algorithm\"\"\"\n",
    "    pcd = np.asarray(input_pcd.points)\n",
    "\n",
    "    inplanes = []\n",
    "    dist_threshold=0.005\n",
    "    ransac_n=20\n",
    "    num_iter=1000\n",
    "\n",
    "    N = len(pcd)\n",
    "    count = 0\n",
    "\n",
    "    while (count < (1 - min_ratio)*N):\n",
    "        pcd = NumpyToPCD(pcd)\n",
    "        plane_model, inliers = pcd.segment_plane(dist_threshold, ransac_n, num_iter)\n",
    "        count += len(inliers)\n",
    "        pcd = np.asarray(pcd.points)\n",
    "        inplanes.append((plane_model, pcd[inliers]))\n",
    "        pcd = np.delete(pcd, inliers, axis=0)\n",
    "    \n",
    "    return inplanes\n",
    "\n",
    "def feature_calc(result):\n",
    "    # Initialize required arrays\n",
    "    planes = []\n",
    "#     colors = []\n",
    "    equations = []\n",
    "    tilt_angle = []\n",
    "\n",
    "    # Target plane to calculate plane angle\n",
    "    xz_plane = np.array([0, 1, 0])\n",
    "\n",
    "    for eq, plane in results:\n",
    "\n",
    "        # Initiate random rgb values for different planes\n",
    "#         r = random.random()\n",
    "#         g = random.random()\n",
    "#         b = random.random()\n",
    "\n",
    "#         color = np.zeros((plane.shape[0], plane.shape[1]))\n",
    "#         color[:, 0] = r\n",
    "#         color[:, 1] = g\n",
    "#         color[:, 2] = b\n",
    "\n",
    "        # Calculating tilt angles of the planes detected\n",
    "        vec1 = [eq[0], eq[1], eq[2]]\n",
    "        num = np.dot(vec1,xz_plane)\n",
    "        denom = np.linalg.norm(vec1)*np.linalg.norm(xz_plane)\n",
    "        theta = np.arccos(num/denom)\n",
    "\n",
    "        # Adding the values to the appropriate arrays\n",
    "        tilt_angle.append(theta)\n",
    "        planes.append(plane)\n",
    "#         colors.append(color)\n",
    "        equations.append(eq)\n",
    "\n",
    "    # Reshaping the array for clustering algorithm\n",
    "    tilt_angle = np.asarray(tilt_angle).reshape(-1,1)\n",
    "\n",
    "    # Using clustering algorithm to identify anomalies\n",
    "    cluster = DBSCAN(eps=0.03,min_samples=1).fit(tilt_angle)\n",
    "    labels = cluster.labels_\n",
    "\n",
    "    # Returning the labels with most counts and identifying its index\n",
    "    label,counts = np.unique(labels,return_counts = True)\n",
    "    index = np.where(counts==np.max(counts))[0][0]\n",
    "    cluster_index = label[index]\n",
    "\n",
    "    # Initializing required arrays\n",
    "    eq_result = []\n",
    "    points = []\n",
    "#     rgb = []\n",
    "    count = 0\n",
    "\n",
    "    # Retrieving the equation of the planes that correspond to labels from clustering algorithm\n",
    "    for i in labels:\n",
    "        if (i == cluster_index):\n",
    "            eq_result.append(equations[count])\n",
    "            points.append(planes[count])\n",
    "#             rgb.append(colors[count])\n",
    "        count += 1\n",
    "\n",
    "    # Concatenating the arrays for visualization purposes\n",
    "    points = np.concatenate(points, axis=0)\n",
    "#     rgb = np.concatenate(rgb, axis=0)\n",
    "\n",
    "    # Visualization of the Point Cloud data\n",
    "    # pcd = o3d.geometry.PointCloud()\n",
    "    # pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    # pcd.colors = o3d.utility.Vector3dVector(rgb)\n",
    "    # o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "    # Calculating the Tread and Riser dimensions\n",
    "    y_mean = np.average(points[:,1])\n",
    "    z_mean = np.average(points[:,2])\n",
    "    y25 = np.percentile(points[:,1],25)\n",
    "    y75 = np.percentile(points[:,1],75)\n",
    "    z25 = np.percentile(points[:,2],25)\n",
    "    z75 = np.percentile(points[:,2],75)\n",
    "\n",
    "    y1 = []\n",
    "    y2 = []\n",
    "    y3 = []\n",
    "    z1 = []\n",
    "    z2 = []\n",
    "    z3 = []\n",
    "\n",
    "    for a,b,c,d in eq_result:\n",
    "        y1.append((-c*z_mean - d)/b) \n",
    "        z1.append((-y_mean*b - d)/c)\n",
    "        y2.append((-c*z25 - d)/b) \n",
    "        z2.append((-y25*b - d)/c)\n",
    "        y3.append((-c*z75 - d)/b) \n",
    "        z3.append((-y75*b - d)/c)\n",
    "    #     print(\"y = {}x + {}\".format(-b/c,-d/c))\n",
    "\n",
    "\n",
    "    tread = ((np.max(z1) - np.min(z1)) + (np.max(z2) - np.min(z2)) + (np.max(z3) - np.min(z3))) * (1/3)\n",
    "    riser = ((np.max(y1) - np.min(y1)) + (np.max(y2) - np.min(y2)) + (np.max(y3) - np.min(y3))) * (1/3)\n",
    "    \n",
    "    return tread,riser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4350e0d7",
   "metadata": {},
   "source": [
    "# Read Bag File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f14ae4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Streaming Stopped.\n"
     ]
    }
   ],
   "source": [
    "bagfilename = \"case3.bag\"\n",
    "\n",
    "try:\n",
    "    # Create pipeline\n",
    "    pipeline = rs.pipeline()\n",
    "\n",
    "    # Create a config object\n",
    "    config = rs.config()\n",
    "\n",
    "    # Tell config that we will use a recorded device from file to be used by the pipeline through playback.\n",
    "    rs.config.enable_device_from_file(config, bagfilename)\n",
    "\n",
    "    # Configure the pipeline to stream the depth stream\n",
    "    # Change this parameters according to the recorded bag file resolution\n",
    "    config.enable_stream(rs.stream.depth, rs.format.z16, 30)\n",
    "    config.enable_stream(rs.stream.color, rs.format.rgb8, 30)\n",
    "\n",
    "    # Start streaming from file\n",
    "    pipeline.start(config)\n",
    "\n",
    "    # Create opencv window to render image in\n",
    "    cv2.namedWindow(\"Intel Realsense\", cv2.WINDOW_AUTOSIZE)\n",
    "    \n",
    "    # Create colorizer object\n",
    "    colorizer = rs.colorizer()\n",
    "\n",
    "    # Streaming loop\n",
    "    while True:\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        align_to = rs.stream.color\n",
    "        align = rs.align(align_to)\n",
    "        aligned_frames = align.process(frames)\n",
    "        \n",
    "        # Obtain Camera Intrinsic Parameters \n",
    "        intrinsics = aligned_frames.profile.as_video_stream_profile().intrinsics\n",
    "        pinhole_camera_intrinsic = o3d.camera.PinholeCameraIntrinsic(\n",
    "            intrinsics.width, intrinsics.height, intrinsics.fx, intrinsics.fy, intrinsics.ppx, intrinsics.ppy)\n",
    "        \n",
    "        # Wait for a coherent pair of frames: depth and color\n",
    "        depth_frame = aligned_frames.get_depth_frame()\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "        if not depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        # Convert images to numpy arrays\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "        # Perform object detection on color image from camera stream\n",
    "        input_tensor = tf.convert_to_tensor(np.expand_dims(color_image, 0), dtype=tf.float32)\n",
    "        detections = detect_fn(input_tensor)\n",
    "\n",
    "        num_detections = int(detections.pop('num_detections'))\n",
    "        detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "        detections['num_detections'] = num_detections\n",
    "        detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "        label_id_offset = 1\n",
    "        image_np_with_detections = color_image.copy()\n",
    "\n",
    "        viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "                    image_np_with_detections,\n",
    "                    detections['detection_boxes'],\n",
    "                    detections['detection_classes']+label_id_offset,\n",
    "                    detections['detection_scores'],\n",
    "                    category_index,\n",
    "                    use_normalized_coordinates=True,\n",
    "                    max_boxes_to_draw=1,\n",
    "                    min_score_thresh=.5,    # Change this threshold value as needed\n",
    "                    agnostic_mode=False)    \n",
    "        \n",
    "        height, width, channels = color_image.shape\n",
    "        ymin, xmin, ymax, xmax = detections['detection_boxes'][0]\n",
    "        bbox = [int(ymin*height), int(xmin*width), int(ymax*height), int(xmax*width)]\n",
    "        \n",
    "        if (detections['detection_scores'][0] > 0.85):\n",
    "            print(\"[INFO] Stairs Detected\")\n",
    "\n",
    "            # Crop RGB Image\n",
    "            color_crop_img_np = color_image[bbox[0]:bbox[2], bbox[1]:bbox[3]]\n",
    "            cv2.imwrite('test_color.png',color_crop_img_np)\n",
    "\n",
    "            # Crop Depth Image\n",
    "            depth_crop_img_np = depth_image[bbox[0]:bbox[2], bbox[1]:bbox[3]]\n",
    "            cv2.imwrite('test_depth.png',depth_crop_img_np)\n",
    "            \n",
    "            # Feature Extractopm\n",
    "            img_color = o3d.io.read_image('test_color.png')\n",
    "            img_depth = o3d.io.read_image('test_depth.png')\n",
    "\n",
    "            #Create RGBD image\n",
    "            rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(img_color, img_depth)\n",
    "\n",
    "\n",
    "            # Create Point Cloud\n",
    "            pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd_image, pinhole_camera_intrinsic)\n",
    "\n",
    "\n",
    "            # Downsample the point cloud\n",
    "            pcd = pcd.voxel_down_sample(voxel_size=0.04)\n",
    "\n",
    "            counter = 0\n",
    "\n",
    "            # Initialize parameters \n",
    "            min_ratio = 0.1\n",
    "\n",
    "            # Using RANSAC to detect planes \n",
    "            results = detect_planes(pcd, min_ratio)\n",
    "\n",
    "            # Dimension calculation\n",
    "            tread, riser = feature_calc(results)\n",
    "\n",
    "\n",
    "            print(\"[INFO] Tread dimensions = %.2f m\"%tread)\n",
    "            print(\"[INFO] Riser dimensions = %.2f m\"%riser)\n",
    "            \n",
    "\n",
    "        cv2.namedWindow('RealSense',cv2.WINDOW_NORMAL)\n",
    "        cv2.imshow('RealSense',image_np_with_detections)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key & 0xFF == ord('q') or key == 27:\n",
    "            cv2.destroyAllWindows()\n",
    "            break  \n",
    "\n",
    "finally:\n",
    "    pipeline.stop()\n",
    "    print(\"[INFO] Streaming Stopped.\")\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8928beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_color = o3d.io.read_image('test_color.png')\n",
    "img_depth = o3d.io.read_image('test_depth.png')\n",
    "\n",
    "#Create RGBD image\n",
    "rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(img_color, img_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3f5213",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagfilename = \"case3.bag\"\n",
    "\n",
    "try:\n",
    "    # Create pipeline\n",
    "    pipeline = rs.pipeline()\n",
    "\n",
    "    # Create a config object\n",
    "    config = rs.config()\n",
    "\n",
    "    # Tell config that we will use a recorded device from file to be used by the pipeline through playback.\n",
    "    rs.config.enable_device_from_file(config, bagfilename)\n",
    "\n",
    "    # Configure the pipeline to stream the depth stream\n",
    "    # Change this parameters according to the recorded bag file resolution\n",
    "    config.enable_stream(rs.stream.depth, rs.format.z16, 30)\n",
    "    config.enable_stream(rs.stream.color, rs.format.rgb8, 30)\n",
    "\n",
    "    # Start streaming from file\n",
    "    pipeline.start(config)\n",
    "\n",
    "    # Create opencv window to render image in\n",
    "    cv2.namedWindow(\"Intel Realsense\", cv2.WINDOW_AUTOSIZE)\n",
    "    \n",
    "    # Create colorizer object\n",
    "    colorizer = rs.colorizer()\n",
    "\n",
    "    # Streaming loop\n",
    "    while True:\n",
    "         # Wait for a coherent pair of frames: depth and color\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        if not depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        # Convert images to numpy arrays\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "        color_img = np.asanyarray(color_frame.get_data())\n",
    "        color_image = cv2.cvtColor(color_img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Apply colormap on depth image (image must be converted to 8-bit per pixel first)\n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "        depth_colormap_dim = depth_colormap.shape\n",
    "            \n",
    "        # Perform object detection on color image from camera stream\n",
    "        input_tensor = tf.convert_to_tensor(np.expand_dims(color_image, 0), dtype=tf.float32)\n",
    "        detections = detect_fn(input_tensor)\n",
    "\n",
    "        num_detections = int(detections.pop('num_detections'))\n",
    "        detections = {key: value[0, :num_detections].numpy()\n",
    "                      for key, value in detections.items()}\n",
    "        detections['num_detections'] = num_detections\n",
    "        detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "        label_id_offset = 1\n",
    "        image_np_with_detections = color_image.copy()\n",
    "\n",
    "        viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "                    image_np_with_detections,\n",
    "                    detections['detection_boxes'],\n",
    "                    detections['detection_classes']+label_id_offset,\n",
    "                    detections['detection_scores'],\n",
    "                    category_index,\n",
    "                    use_normalized_coordinates=True,\n",
    "                    max_boxes_to_draw=5,\n",
    "                    min_score_thresh=.5,    # Change this threshold value as needed\n",
    "                    agnostic_mode=False)    \n",
    "            \n",
    "        color_colormap_dim = image_np_with_detections.shape\n",
    "\n",
    "        # If depth and color resolutions are different, resize color image to match depth image for display\n",
    "        if depth_colormap_dim != color_colormap_dim:\n",
    "            resized_color_image = cv2.resize(image_np_with_detections, dsize=(depth_colormap_dim[1], depth_colormap_dim[0]), interpolation=cv2.INTER_AREA)\n",
    "            images = np.hstack((resized_color_image, depth_colormap))\n",
    "        else:\n",
    "            images = np.hstack((color_image, depth_colormap))\n",
    "\n",
    "        cv2.imwrite('case3_rgb.png',color_image)\n",
    "        cv2.imwrite('case3_colored_depth.png',depth_colormap)\n",
    "        cv2.imwrite('case3_depth.png',depth_image)\n",
    "        # Show images\n",
    "        cv2.namedWindow('RealSense', cv2.WINDOW_NORMAL)\n",
    "        cv2.imshow('RealSense', images)\n",
    "        key = cv2.waitKey(1)\n",
    "        # if pressed escape exit program\n",
    "        if key == 27:\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052eb7af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfod",
   "language": "python",
   "name": "tfod"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
